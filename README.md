Some material from my book <b>Statistical Optimization for AI and Machine Learning</b>, availabele <a href="https://mltechniques.com/shop/">here</a>. In particular:
<ul>
  <li>My gradient descent technique implemented in <code>gradient.py</code>, available <a href="https://github.com/VincentGranville/Experimental-Math-Number-Theory/tree/main/Source-Code">in this folder</a>. 
  <li>The <code>interpol.py</code>, <code>interpol_fourier.py</code> and <code>interpol_ortho.py</code> programs in this folder are described in my article <em>New Interpolation Methods for Data Synthetization and Prediction</em>, available <a href="https://mltechniques.com/2023/01/14/new-interpolation-methods-for-synthetization-and-prediction/">here</a>.
  <li> For feature clustering, see <code>featureClustering.py</code> and <code>featureClusteringScipy.py</code> (the latter with hierarchical clustering) <a href="https://github.com/VincentGranville/Main">in this folder</a>.
 <li>Fast grid search for faster hyperparameter tuning: see <code>ZetaGeom.py</code> in this folder. The article describing and documenting the method is available <a href="https://mltechniques.com/2023/03/30/smart-grid-search-case-study-with-hybrid-zeta-geometric-distributions-and-synthetic-data/">here</a>. 
 <li>Stochastic thinning: new technique to boost learning algorithms. See <code>thinning_neuralNets.py</code>, and <code>thinning_regression.py</code> in this folder. The article describing and documenting the method will soon be available <a href="https://mltechniques.com/resources/">here</a>. 
 <li> Material about Generative Adversarial Networks (GAN), NoGAN and NoGAN2, is in the main folder. The corresponding Python libraries are described in the book.
</ul>
